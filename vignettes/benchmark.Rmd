---
title: "Solver Benchmarks"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: true
    self_contained: yes
fontsize: 11pt
documentclass: article
bibliography: references.bib
csl: reference-style.csl
vignette: >
  %\VignetteIndexEntry{Solver benchmarks}
  %\VignetteEngine{knitr::rmarkdown_notangle}
---

```{r, include = FALSE}
w <- 7
h <- 4
is_check <- ("CheckExEnv" %in% search()) || any(c("_R_CHECK_TIMINGS_",
             "_R_CHECK_LICENSE_") %in% names(Sys.getenv()))
knitr::opts_chunk$set(fig.align = "center", eval = !is_check,
                      fig.width = w, fig.height = h)
```

```{r, include = FALSE}
devtools::load_all()
```

# Introduction

The purpose of this vignette is to characterize the performance of different solvers that are currently supported by the _prioritizr_ R package. It is intended to help users understand the size (e.g. number of planning units) and complexity (e.g. adding boundary length penalties to reduce spatial fragmentation) of conservation planning problems that different solvers can tackle. Additionally, since the two fastest solvers are based on commercial software (i.e. _IBM CPLEX_ and _Gurobi_), it is also intended to help users decide if the potential benefits of purchasing a license for one of these solvers are worthwhile. **Note that special academic licenses are available for these _IBM CPLEX_ and _Gurobi_ at no cost.** Although the solvers based on open source software (i.e. _SYMPHONY_ and _CBC_) may not be as fast as the solvers based on commercial software (i.e. _IBM CPLEX_ and _Gurobi_), they may still be useful depending on the size and complexity of the conservation planning problem. Indeed -- as you will see when reading through this vignette -- the _CBC_ software performs relatively well for certain types of conservation planning problems.

To start off, we will the load packages used in this vignette.

```{r "load packages", message = FALSE}
# load packages
library(prioritizr)
library(piggyback)
library(ggplot2)
library(units)
library(tidyverse)
```

# Methods

This vignette will report the results of a benchmark analysis. To reduce computational burden, we previously completed the benchmark analysis ([code available online](https://github.com/prioritizr/benchmark)) and [uploaded the results to an online repository](https://github.com/prioritizr/benchmark/releases). This analysis involved generating prioritizations using different solvers and recording how long it took for the solvers to finish. To help understand the factors that influence how long it takes for solvers to generate a prioritization, we examined a suite of conservation planning problems with varying size (i.e. number of planning units), complexity (i.e. varying penalties to reduce spatial fragmentation), and with different objective functions (i.e. [metric used to evaluate competing solutions](https://prioritizr.net/reference/objectives.html)). In this section, we will download the results the previously completed benchmark analysis and examine the parameters used to conduct it.

## Download benchmark results

Let's download the results of the benchmark analysis. This code will save the results to a temporary folder on your computer. Please note that downloading the results might take several minutes to complete depending on your Internet connection. If you are unable to download the results onto your computer, you can simply view the graphs shown in the Results section.

```{r "import results", message = FALSE}
# download data to temporary folder
pb_download(
  file = c("solutions.zip", "results.rda"),
  repo = "prioritizr/benchmark", dest = tempdir(), tag = "latest",
  show_progress = FALSE)

# load benchmark results
load(file.path(tempdir(), "results.rda"))

# load benchmark solutions
unzip(file.path(tempdir(), "solutions.zip"), exdir = tempdir())
solution_paths <-
  file.path(tempdir(), "solutions", paste0(benchmark_results$id, ".tif"))
solution_raster_data <- lapply(solution_paths, raster)
```

## Benchmark parameters

After downloading the benchmark results, let's have a look at the parameters that were used to conduct it. Note that all benchmark scenarios have 72 features.

```{r "benchmark parameters"}
# number of features (e.g. number of different species examined)
unique(benchmark_results$number_features)

# representation targets,
# units are proportion of the total amount of each feature (e.g. 0.1 = 10%)
unique(benchmark_results$relative_target)

# number of planning units
unique(benchmark_results$number_of_planning_units)

# objective functions
unique(benchmark_results$objective)

# boundary penalty values,
# note that different values were examined for different objective functions

## boundary penalty values for min set objective function
boundary_penalty_values$add_min_set_objective

## boundary penalty values for min shortfall objective function
boundary_penalty_values$add_min_shortfall_objective

# budgets examined for budget-limited objectives (e.g. 0.1 = 10% of total cost)
## note that the min set objective function does not use a budget,
## and thus it has a NA value
tibble(objective = unique(benchmark_results$objective),
       budget = unique(benchmark_results$budget))

```

## Helper functions

Now we will define some helper variables to store information on the different benchmark runs. We will also define a helper function to plot the results from the benchmark analysis. This will be helpful for interpreting the results of the benchmark analysis in the following section.

```{r "preliminary calculations", message = FALSE}
# extract different numbers of planning units examined the benchmark analysis
n_planning_units <- unique(benchmark_results$number_of_planning_units)

# extract boundary penalties examined under different objectives
boundary_penalty_values <-
  benchmark_results %>%
  plyr::dlply("objective", function(x) unique(x$boundary_penalty))

# define helper function to create plots
plot_benchmark <- function(
  objective, n_pu, boundary_penalty, solver = NULL){
  # assert arguments are valid
  ## verify parameters with no default arguments
  assertthat::assert_that(
    assertthat::is.count(n_pu), assertthat::noNA(n_pu),
    n_pu %in% unique(benchmark_results$number_of_planning_units),
    assertthat::is.number(boundary_penalty), assertthat::noNA(boundary_penalty),
    assertthat::is.string(objective), assertthat::noNA(objective),
    objective %in% unique(benchmark_results$objective))
  ## set default argument for solver if needed
  if (is.null(solver)) {
    solver <- unique(benchmark_results$solver)
  }
  ## verify solver argument
  assertthat::assert_that(
    is.character(solver), all(solver %in% benchmark_results$solver))
  ## verify that only a single set of features was used
  assertthat::assert_that(
    dplyr::n_distinct(benchmark_results$number_features) == 1)

  # prepare data for plotting
  ## rename variables to avoid scoping issues
  sol <- solver
  obj <- objective
  bp <- boundary_penalty
  ## subset data relevant for plotting
  plot_data <-
    benchmark_results %>%
    filter(.$objective == obj, .$solver %in% sol,
           .$number_of_planning_units == n_pu,
           .$boundary_penalty == bp)
  ## scale run time to helpful units for plotting
  plot_units <-
    dplyr::case_when(
      # show hours if max(run_time) > 3 h
      max(plot_data$run_time) > 60 * 60 * 3 ~ "hours",
      # show minutes if max(run_time) > 3 M
      max(plot_data$run_time) > 60 * 3 ~ "minutes",
      # else show seconds
      TRUE ~ "seconds")
   plot_data$run_time_scaled <-
    plot_data$run_time %>%
    units::set_units(s) %>%
    units::set_units(plot_units, mode = "standard") %>%
    as.numeric()
  ## plot labels
  n_f <- unique(benchmark_results$number_features)[1]
  plot_title =
    paste0(
      dplyr::case_when(
        objective == "add_min_set_objective" ~ "Min. set",
        objective == "add_min_shortfall_objective" ~ "Min. shortfall",
        TRUE ~ objective),
      ": ",
      formatC(
        n_f, big.mark = ",", digits = 2, format = "f",
        drop0trailing = TRUE),
      " features, ",
      formatC(
        n_pu, big.mark = ",", digits = 2, format = "f",
        drop0trailing = TRUE),
      " planning units")
  if (bp > 1e-15) {
    plot_title <- paste0(plot_title, ", ", bp, " boundary penalty")
  }
  ## determine colors for solvers (so that solvers always have same color
  solver_names <- unique(benchmark_results$solver)
  solver_colors <- scales::hue_pal()(length(solver_names))
  names(solver_colors) <- solver_names

  # return plot for selected benchmark runs
  ggplot(
    data = plot_data,
    mapping = aes(x = relative_target, y = run_time_scaled, color = solver)) +
  scale_y_continuous(limits = c(0, NA_real_)) +
  geom_line() +
  geom_point() +
  scale_color_manual(values = solver_colors) +
  labs(
    title = plot_title,
    x = "Representation target (%)",
    y = paste0("Run time (", plot_units, ")"))
}
```

# Results

We will now inspect the results of the benchmark analysis. The `benchmark_results` object is a table (i.e. `tibble()`) containing information for each benchmark run (e.g. run time), and the `solution_raster_data` object (i.e. `RasterStack`) contains the prioritizations generated for each benchmark run.

```{r "preview results"}
# preview results
print(benchmark_results)
```

let's examine the average run time for each solver across all benchmark runs. To achieve this, we will create a box plot. Since we ideally want solvers that generate prioritizations quickly, solvers that have shorter (lower) run times are considered better. When looking at this box plot, we can see that average run times for the _CPLEX_ and _Gurobi_ solvers are consistently low and don't differ much from each other. Additionally, the run times for the open source _CBC_ solver are generally low too. However, there are some cases where the run times for the _CBC_ solver are much higher than that for the _CPLEX_ and _Gurobi_ solvers. We can also see that the _lpsymphony_ and _Rsymphony_ solvers -- which both use the open source _SYMPHONY_ software -- generally take much longer to generate prioritizations than the other solvers.

```{r "plot average run times"}
# plot overall summary of solver performance
ggplot(
  data =
    benchmark_results %>%
    mutate(
      run_time_scaled = as.numeric(set_units(set_units(
        run_time, "seconds"), "hours"))),
  aes(x = solver, y = run_time_scaled)) +
geom_boxplot() +
theme(axis.text.x = element_text(size = 7)) +
labs(x = "Solver", y = "Run time (hours)")
```

Now, let's investigate the solver behavior in more detail. Specifically, we examine the benchmark results generated for the minimum set objective function. This is because the minimum set objective function is the most commonly used objective function in systematic conservation planning.

## Minimum set results (no boundary penalty)

Let's start with the results for the smallest and simplest conservation planning problems examined in the benchmark analysis. Here, all prioritizations were generated using problems that involved `r n_planning_units[1]` planning units and did not contain any boundary length penalties. Since all benchmark scenarios have 72 features -- as mentioned earlier -- all of these prioritizations were generated with 72 features. When looking at the results, we can see that all solvers solve the problem in a comparable amount of time across all targets investigated.

```{r "time for pu's 1"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = 0)
```

Next, let's look at the results for a more realistic problem involving `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares. Note that all other factors (e.g. absence of boundary length penalties) are the same as for the previous graph.

```{r "time for pu's 2"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = 0)
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Now we really start to see the difference between the _Rsymphony_ and _lpsymphony_ solvers -- which use the _SYMPHONY_ software -- and the other solvers.

```{r "time for pu's 3"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = 0)
```

Finally, let's look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. We can see that both the _Rsymphony_ and _lpsymphony_ solvers are really taking a much longer time to generate prioritizations now.

```{r "time for pu's 4"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = 0)
```

To get a better sense of how the faster solvers (i.e. based on _CBC_, _IBM CPLEX_, _Gurobi_) compare for such a large problem, let's take look a closer look at these three solvers. Interestingly, we can see that the solver based on the open source _CBC_ software is slightly faster -- by a few seconds -- than the other solvers.

```{r "time for pu's 4 fast only"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = 0,
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```


## Minimum set results with low boundary penalty

Now let's at the same problem types, but this time with a low boundary length penalty value added to the problem formulation. To start with, we will look at scenarios with a low `boundary_penalty` value of `r boundary_penalty_values$add_min_set_objective[2]`. Let's start again with the smallest problem size we've benchmarked. This problem has only `r n_planning_units[1]` planning units.

```{r "time for pu's 1 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[2])
```

Next, let's look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares.

```{r "time for pu's 2 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[2])
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Now we really start to see the difference between _lpsymphony_ and _Rsymphony_ solvers and the other solvers.

```{r "time for pu's 3 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Finally, let's look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. As with the scenario without boundary penalties, _lpsymphony_ and _Rsymphony_ solvers take a lot longer to generate prioritizations than the other three solvers.

```{r "time for pu's 4 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[2])
```

Similar to earlier, let's take a look at just the _CBC_, _IBM CPLEX_, and _Gurobi_ solvers. We can see that the _Gurobi_ solver has the best performance, generating generating prioritizations in under half an hour in all cases. This result shows that the commercial solvers can massively outperform the open source solvers for large-scale problems with boundary length penalties.

```{r "time for pu's 4 with low boundary penalty, fast solvers"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[2],
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```

## Minimum set results with high boundary penalty

Now let's look at the same problem types, but this time with a high boundary length penalty parameter added to the problem formulation (i.e. `r boundary_penalty_values$add_min_set_objective[3]`). Let's start again with the smallest problem size we've benchmarked. This problem has only `r n_planning_units[1]` planning units.

```{r "time for pu's 1 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Next, let's look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares.

```{r "time for pu's 2 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. We again see the difference between _Rsymphony_ and _lpsymphony_ solvers and the other solvers.

```{r "time for pu's 3 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Finally, let's look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. As with the previous scenarios, _Rsymphony_ and _lpsymphony_ solvers take a lot longer to generate prioritizations than the other solvers.

```{r "time for pu's 4 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Similar to before, let's take a look at just the _CBC_, _IBM CPLEX_, and _Gurobi_ solvers. We can see that _Gurobi_ has the best performance. This result further emphasizes the potential speed gains of commercial solvers for large-scale conservation planning problems with boundary penalties.

```{r "time for pu's 4 with high boundary penalty, fast solvers"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3],
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```

## Minimize shortfall results (no boundary penalty)

Now, let's investigate the solver behavior for the min shortfall objective function. Let's start with the smallest problem size we've benchmarked. All benchmark scenarios have 72 features. This problem has only `r n_planning_units[1]` planning units. You can see that all solvers solve the problem in a comparable amount of time across all targets investigated. Only the _CBC_ solver takes longer then the other ones.

```{r "min_short time for pu's 1"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = 0)
```

Next, let's look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares. The _CBC_ solver takes longer than the other solvers again.

```{r "min_short time for pu's 2"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = 0)
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Now we really start to see the difference between the _CBC_ solver and the other solvers, with the _CBC_ solver taking many more minutes to complete.

```{r "min_short time for pu's 3"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = 0)
```

Finally, let's look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. We can see that the open source _CBC_, _Rsymphony_ and _lpsymphony_ solvers now take a lot longer than the commercial _IBM CPLEX_ and _Guorbi_ solvers.

```{r "min_short time for pu's 4"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = 0)
```

## Minimize shortfall results with low boundary penalty

Now let's look at the same problem type, but this time with a low `boundary_penalty` parameter added to the problem formulation (i.e. `r boundary_penalty_values$add_min_shortfall_objective[2]`). Let's start again with the smallest problem size we've benchmarked. This problem has only `r n_planning_units[1]` planning units.

```{r "min_short time for pu's 1 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2])
```

Next, let's look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares. We can see the _CBC_ solver takes longer than the other solvers again.

```{r "min_short time for pu's 2 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2])
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Now we really start to see the difference between the _CBC_ solver and the other solvers. Additionally, the _Rsymphony_ and _lpsymphony_ solvers also also take considerably longer than the commercial solvers too.

```{r "min_short time for pu's 3 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2])
```

Finally, let's look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. The _CBC_ solver now performs better than the _Rsymphony_ and _lpsymphony_ solvers, which take approximately one day to solve this problem.

```{r "min_short time for pu's 4 and low boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2])
```

To get a better sense of how the faster solvers compare (i.e. _CBC_, _IBM CPLEX_, and _Gurobi_), let's take a closer look at these three solvers. We can see that the _CBC_ solver takes a lot longer to generate prioritizations than the _IBM CPLEX_ and _Gurobi_ solvers. This result suggests that it _IBM CPLEX_ and _Gurobi_ could be really beneficial for large-scale conservation planning problems with boundary length penalties and the minimum shortfall objective function.

```{r "min_short time for pu's 4 and boundary penalty fast"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2],
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```

## Minimize shortfall results with high boundary penalty

Now let's look at the same problem types, but this time with a higher `boundary_penalty` parameter added to the problem formulation (`r boundary_penalty_values$add_min_shortfall_objective[3]`). Let's start again with the smallest problem size we've benchmarked. This problem has only `r n_planning_units[1]` planning units.

```{r "min_short time for pu's 1 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3])
```

Next, let's look at the results for a greater number of planning units (i.e. with `r n_planning_units[2]` planning units) and see how the timings for the different solvers compare. All the solvers have a similar run time. Interestingly, the _Gurobi_ solver is the slowest -- but only by a couple of minutes -- for these benchmark parameters.

```{r "min_short time for pu's 2 with boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3])
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Similar to some of the results observed in the previous section, the _CBC_ solver is the slowest and the _IBM CPLEX_ and _Gurobi_ commercial solvers are fastest for the minimum shortfall objective.

```{r "min_short time for pu's 3 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3])
```

Finally, let's look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. We can see that the benchmark times vary greatly for the _Rsymphony_ and _lpsymphony_ solvers, with run times ranging from one to two whole days. Additionally, although the _CBC_ solver can still require several hours to generate a prioritization, it now outperforms better than the the _Rsymphony_ and _lpsymphony_ solvers. Finally, the _IBM CPLEX_ and _Gurobi_ solvers perform much, much better than all the other solves.

```{r "min_short time for pu's 4 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3])
```

# Conclusion

Richard, could you please write a short conclusion paragraph with some recommendations? E.g. do we recommend using the CBC solver first, and if that doesn't work, try using lpsymphony solver?
