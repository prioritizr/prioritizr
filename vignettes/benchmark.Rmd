---
title: "Solver Benchmarks"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: true
    self_contained: yes
fontsize: 11pt
documentclass: article
bibliography: references.bib
csl: reference-style.csl
vignette: >
  %\VignetteIndexEntry{Solver benchmarks}
  %\VignetteEngine{knitr::rmarkdown_notangle}
---

```{r, include = FALSE}
w <- 7
h <- 4
is_check <- ("CheckExEnv" %in% search()) || any(c("_R_CHECK_TIMINGS_",
             "_R_CHECK_LICENSE_") %in% names(Sys.getenv()))
knitr::opts_chunk$set(fig.align = "center", eval = !is_check,
                      fig.width = w, fig.height = h)
```

```{r, include = FALSE}
devtools::load_all()
```

# Introduction

The purpose of this vignette is to compare the different solvers we currently support in the _prioritizr_ R package, in terms of how long it takes them to solve problems of varying level of complexity. First, we test the most widely used objective function in systematic conservation planning to date, the [add_min_set](https://prioritizr.net/reference/add_min_set_objective.html) objective. Second, we test an objective function that adds a budget component to the problem, specifically the [add_min_shortfall](https://prioritizr.net/reference/add_min_shortfall_objective.html) objective.

This vignette is meant to help users decide which solver to use, which can be an important consideration of using _prioritizr_, because two of the fast solvers than can be used are commercial (CPLEX, Gurobi). They are free to use for academic research and teaching, but users outside of academia might have to pay license fees to use them and have to way the pros and cons of doing so. The open source solvers _prioritizr_ currently supports are CBC and SYMPHONY.

As you will see when going through this vignette, the open source CBC solver performs well for problems using the [add_min_set](https://prioritizr.net/reference/add_min_set_objective.html) objective function, especially when no penalties are included. Testing a more complex objective function ([add_min_shortfall](https://prioritizr.net/reference/add_min_shortfall_objective.html)) showed that the commercial solvers outperform the open source solvers, at times considerably.

# Methods

To start off, we will load some packages to help download the benchmark results and visualize them.

```{r "load packages", message = FALSE}
# load packages
library(prioritizr)
library(piggyback)
library(ggplot2)
library(units)
library(tidyverse)
```

Next, download the benchmark results and load them into our session.

```{r "import results", message = FALSE}
# download data to temporary folder
pb_download(
  file = c("solutions.zip", "results.rda"),
  repo = "prioritizr/benchmark", dest = tempdir(), tag = "latest",
  show_progress = FALSE)

# load benchmark results
load(file.path(tempdir(), "results.rda"))

# load benchmark solutions
unzip(file.path(tempdir(), "solutions.zip"), exdir = tempdir())
solution_paths <-
  file.path(tempdir(), "solutions", paste0(benchmark_results$id, ".tif"))
solution_raster_data <- lapply(solution_paths, raster)
```

Now we will define some helper variables to store information on the different benchmark runs. We will also define a helper function to quickly plot the results from the benchmark analysis.

```{r "preliminary calculations", message = FALSE}
# extract different numbers of planning units examined the benchmark analysis
n_planning_units <- unique(benchmark_results$number_of_planning_units)

# extract boundary penalties examined under different objectives
boundary_penalty_values <-
  benchmark_results %>%
  plyr::dlply("objective", function(x) unique(x$boundary_penalty))

# define helper function to create plots
plot_benchmark <- function(
  objective, n_pu, boundary_penalty, solver = NULL){
  # assert arguments are valid
  ## verify parameters with no default arguments
  assertthat::assert_that(
    assertthat::is.count(n_pu), assertthat::noNA(n_pu),
    n_pu %in% unique(benchmark_results$number_of_planning_units),
    assertthat::is.number(boundary_penalty), assertthat::noNA(boundary_penalty),
    assertthat::is.string(objective), assertthat::noNA(objective),
    objective %in% unique(benchmark_results$objective))
  ## set default argument for solver if needed
  if (is.null(solver)) {
    solver <- unique(benchmark_results$solver)
  }
  ## verify solver argument
  assertthat::assert_that(
    is.character(solver), all(solver %in% benchmark_results$solver))
  ## verify that only a single set of features was used
  assertthat::assert_that(
    dplyr::n_distinct(benchmark_results$number_features) == 1)

  # prepare data for plotting
  ## rename variables to avoid scoping issues
  sol <- solver
  obj <- objective
  bp <- boundary_penalty
  ## subset data relevant for plotting
  plot_data <-
    benchmark_results %>%
    filter(.$objective == obj, .$solver %in% sol,
           .$number_of_planning_units == n_pu,
           .$boundary_penalty == bp)
  ## scale run time to helpful units for plotting
  plot_units <-
    dplyr::case_when(
      # show hours if max(run_time) > 3 h
      max(plot_data$run_time) > 60 * 60 * 3 ~ "hours",
      # show minutes if max(run_time) > 3 M
      max(plot_data$run_time) > 60 * 3 ~ "minutes",
      # else show seconds
      TRUE ~ "seconds")
   plot_data$run_time_scaled <-
    plot_data$run_time %>%
    units::set_units(s) %>%
    units::set_units(plot_units, mode = "standard") %>%
    as.numeric()
  ## plot labels
  n_f <- unique(benchmark_results$number_features)[1]
  plot_title =
    paste0(
      dplyr::case_when(
        objective == "add_min_set_objective" ~ "Min. set",
        objective == "add_min_shortfall_objective" ~ "Min. shortfall",
        TRUE ~ objective),
      ": ",
      formatC(
        n_f, big.mark = ",", digits = 2, format = "f",
        drop0trailing = TRUE),
      " features, ",
      formatC(
        n_pu, big.mark = ",", digits = 2, format = "f",
        drop0trailing = TRUE),
      " planning units")
  if (bp > 1e-15) {
    plot_title <- paste0(plot_title, ", ", bp, " boundary penalty")
  }
  ## determine colors for solvers (so that solvers always have same color
  solver_names <- unique(benchmark_results$solver)
  solver_colors <- scales::hue_pal()(length(solver_names))
  names(solver_colors) <- solver_names

  # return plot for selected benchmark runs
  ggplot(
    data = plot_data,
    mapping = aes(x = relative_target, y = run_time_scaled, color = solver)) +
  scale_y_continuous(limits = c(0, NA_real_)) +
  geom_line() +
  geom_point() +
  scale_color_manual(values = solver_colors) +
  labs(
    title = plot_title,
    x = "Representation target (%)",
    y = paste0("Run time (", plot_units, ")"))
}
```

## Benchmark parameters used

Lets have a look at the parameters that were used in the benchmark scenarios. If you want to explore the full benchmarking code and analysis, you can find the GitHub repository here: [benchmark repo](https://github.com/prioritizr/benchmark).

```{r "benchmark parameters"}
# number of features
unique(benchmark_results$number_features)

# targets tested
unique(benchmark_results$relative_target)

# number of planning units
unique(benchmark_results$number_of_planning_units)

# objective functions tested
unique(benchmark_results$objective)

# boundary penalty values for min set
boundary_penalty_values$add_min_set_objective

# boundary penalty values for min shortfall
boundary_penalty_values$add_min_shortfall_objective

# budget for budget-limited objectives (0.1 = 10% of total cost)
tibble(objective = unique(benchmark_results$objective),
       budget = unique(benchmark_results$budget))

```

# Results

We can now inspect the benchmark results. The `benchmark_results` table contains information for each benchmark run (e.g. run time), and the `solution_raster_data` contains the solutions generated for each benchmark run.

```{r "preview results"}
# preview results
print(benchmark_results)
```

Lets now generate a plot that shows average run times per solver. You can see that average run times for CPLEX and Gurobi are consistently low and don't differ much from each other. CBC run times are generally low, but there are some cases where run time is significantly longer than for CPLEX and Gurobi. lpsymphony and Rsymphony, which both use
the SYMPHONY solver, take longer on average and with more complex problems, the time difference between them and the other solvers is substantial.

```{r "plot average run times"}
# plot overall summary of solver performance
ggplot(
  data =
    benchmark_results %>%
    mutate(
      run_time_scaled = as.numeric(set_units(set_units(
        run_time, "seconds"), "hours"))),
  aes(x = solver, y = run_time_scaled)) +
geom_boxplot() +
theme(axis.text.x = element_text(size = 7)) +
labs(x = "Solver", y = "Run time (hours)")
```

## Minimum set results (no boundary penalty)

Now, lets investigate the solver behavior in more detail. Let's start with the smallest problem size we've benchmarked. This first set of comparisons is done without including penalties or constraints (e.g. boundary penalties). All benchmark scenarios have 72 features. This problem has only `r n_planning_units[1]` planning units. You can see that all solvers solve the problem in a comparable amount of time across all targets investigated. 

```{r "time for pu's 1"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = 0)
```

Next, lets look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares.

```{r "time for pu's 2"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = 0)
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Now we really start to see the difference between SYMPHONY and the other solvers.

```{r "time for pu's 3"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = 0)
```

Finally, lets look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. The SYMPHONY solver is really struggling to find solutions now.

```{r "time for pu's 4"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = 0)
```

To get a better sense of how the faster solvers (i.e. CBC, CPLEX, Gurobi) compare for this problem size, lets just look at those 3 solvers in the next plot. The open source CBC solver is considerably faster than the commercial CPLEX and Gurobi solvers for this scenario.

```{r "time for pu's 4 fast only"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = 0,
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```


## Minimum set results with low boundary penalty

Now lets at the same problem types, but this time with a `boundary_penalty` parameter added to the problem formulation. To start with, we will look at scenarios with a low `boundary_penalty` value of `r boundary_penalty_values$add_min_set_objective[2]`. Let's start again with the smallest problem size we've benchmarked. This problem has only `r n_planning_units[1]` planning units.

```{r "time for pu's 1 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[2])
```

Next, lets look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares.

```{r "time for pu's 2 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[2])
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Now we really start to see the difference between SYMPHONY and the other solvers.

```{r "time for pu's 3 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Finally, lets look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. As with the scenario without boundary penalties, SYMPHONY takes a lot longer to find solutions than the other three solvers.

```{r "time for pu's 4 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[2])
```

To get a better sense of how the more consistent solvers (i.e. CBC, CPLEX, Gurobi) compare for this problem size, lets just look at those 3 solvers in the next plot. Gurobi comes out on top, but overall CBC and CPLEX find solutions in a reasonable amount of time as well. This result does indicate though that with more complex problem formulations the commercial solvers, especially Gurobi, seem to gain the upper hand compared to open source solvers.

```{r "time for pu's 4 with low boundary penalty, fast solvers"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[2],
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```

## Minimum set results with high boundary penalty

Now lets look at the same problem types, but this time with a higher boundary_penalty parameter added to the problem formulation (`r boundary_penalty_values$add_min_set_objective[3]`). Let's start again with the smallest problem size we've benchmarked. This problem has only `r n_planning_units[1]` planning units.

```{r "time for pu's 1 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Next, lets look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares.

```{r "time for pu's 2 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. We again see the difference between SYMPHONY and the other solvers.

```{r "time for pu's 3 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

Finally, lets look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. As with the scenarios before, SYMPHONY takes a lot longer to find solutions than the other three solvers.

```{r "time for pu's 4 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3])
```

To get a better sense of how the more consistent solvers (CBC, CPLEX, Gurobi) compare for this problem size, lets just look at those 3 solvers in the next plot.Gurobi comes out on top, similar to the scenarios with low boundary penalty, but again CBC and CPLEX find solutions in a reasonable amount of time as well. 

```{r "time for pu's 4 with high boundary penalty, fast solvers"}
plot_benchmark(
  objective = "add_min_set_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_set_objective[3],
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```

## Minimize shortfall results (no boundary penalty)

Now, lets investigate the solver behavior in more detail for the min shortfall objective function. Let's start with the smallest problem size we've benchmarked. All benchmark scenarios have 72 features. This problem has only `r n_planning_units[1]` planning units. You can see that all solvers solve the problem in a comparable amount of time across all targets investigated. Only the CBC solver takes longer then the other ones.

```{r "min_short time for pu's 1"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = 0)
```

Next, lets look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares. CBC takes longer than the other solvers again.

```{r "min_short time for pu's 2"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = 0)
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Now we really start to see the difference between CBC and the other solvers.

```{r "min_short time for pu's 3"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = 0)
```

Finally, lets look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. The open source solvers (CBC, SYMPHONY) take a lot longer than the commercial solvers to find solutions now.

```{r "min_short time for pu's 4"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = 0)
```

To get a better sense of how the faster solvers (CBC, CPLEX, Gurobi) compare for this problem size, lets just look at those 3 solvers in the next plot. CBC really isn't the solver of choice for this objective function and large problem sizes. If available, CPLEX and Gurobi are the recommended choices.

```{r "min_short time for pu's 4 fast only"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = 0,
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```

## Minimize shortfall results with low boundary penalty

Now lets look at the same problem type, but this time with a `boundary_penalty` parameter added to the problem formulation, first with a low boundart penalty value of `r boundary_penalty_values$add_min_shortfall_objective[2]`. Let's start again with the smallest problem size we've benchmarked. This problem has only `r n_planning_units[1]` planning units.

```{r "min_short time for pu's 1 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2])
```

Next, lets look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares.CBC takes longer than the other solvers again.

```{r "min_short time for pu's 2 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2])
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. Now we really start to see the difference between CBC and the other solvers. The SYMPHONY sover based packages also take considerably longer than the commercial solvers now.

```{r "min_short time for pu's 3 with low boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2])
```

Finally, lets look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. CBC now performs better than the SYMPHONY solver based packages, which take around 1 day to actually solve this large problem.

```{r "min_short time for pu's 4 and low boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2])
```

To get a better sense of how the more consistent solvers (i.e. CBC, CPLEX, Gurobi) compare for this problem size, lets just look at those 3 solvers in the next plot. CBC really isn't the solver of choice for this objective function and large problem sizes. If available, CPLEX and Gurobi are the recommended choices. As far as open source solvers go, CBC is a lot faster than the SYMPHONY based packages though.

```{r "min_short time for pu's 4 and boundary penalty fast"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[2],
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```

## Minimize shortfall results with high boundary penalty

Now lets look at the same problem types, but this time with a higher `boundary_penalty` parameter added to the problem formulation (`r boundary_penalty_values$add_min_shortfall_objective[3]`).

Let's start again with the smallest problem size we've benchmarked. This problem has only `r n_planning_units[1]` planning units.

```{r "min_short time for pu's 1 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[1],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3])
```

Next, lets look at the results for a more realistic problem with `r n_planning_units[2]` planning units and see how the timing of the different solvers used compares. All solvers are preforming similarily, interestingly Gurobi is the slowest for this round of benchmarking.

```{r "min_short time for pu's 2 with boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[2],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3])
```

Next, we will look at a medium sized problem with `r n_planning_units[3]` planning units. As with the previous boundary penalty values, CBC is the slowest and the commercial solvers are fasteed, with SYMPHONY in between.

```{r "min_short time for pu's 3 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[3],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3])
```

Finally, lets look at timing comparisons for a large problem with `r n_planning_units[4]` planning units. You can see that the benchmark times vary greatly for the SYMPHONY solver. CBC now performs better than the SYMPHONY solver based packages, which take up to 2 days to actually solve this large problem. CBC takes a maximum of 6 hours.

```{r "min_short time for pu's 4 with high boundary penalty"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3])
```

To get a better sense of how the more consistent solvers (i.e. CBC, CPLEX, Gurobi) compare for this problem size, lets just look at those 3 solvers in the next plot. CBC really isn't the solver of choice for this objective function and large problem sizes. If available, CPLEX and Gurobi are the recommended choices. As far as open source solvers go, CBC is a lot faster than the SYMPHONY based packages though.

```{r "min_short time for pu's 4 with high boundary penalty, fast solvers"}
plot_benchmark(
  objective = "add_min_shortfall_objective",
  n_pu = n_planning_units[4],
  boundary_penalty = boundary_penalty_values$add_min_shortfall_objective[3],
  solver = c("add_cbc_solver", "add_cplex_solver", "add_gurobi_solver"))
```
