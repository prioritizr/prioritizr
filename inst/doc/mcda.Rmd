---
title: "Multiple-criteria decision analysis"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: true
    self_contained: yes
fontsize: 11pt
documentclass: article
bibliography: references.bib
csl: reference-style.csl
vignette: >
  %\VignetteIndexEntry{Multiple-criteria decision analysis}
  %\VignetteEngine{knitr::rmarkdown_notangle}
---

```{r, include = FALSE}
# define dummy variables so that vignette passes package checks
prelim_penalty <- rep(NA_real_, 100)
threshold <- rep(NA_real_, 100)
```

```{r, include = FALSE}
# define variables for vignette figures and code execution
h <- 3.5
w <- 3.5
is_check <- ("CheckExEnv" %in% search()) || any(c("_R_CHECK_TIMINGS_",
             "_R_CHECK_LICENSE_") %in% names(Sys.getenv()))
knitr::opts_chunk$set(fig.align = "center", eval = !is_check)
```

## Introduction

Systematic conservation planning requires making trade-offs [@r4; @r35]. Since different criteria may conflict with one another -- or not align perfectly -- prioritizations need to trade-offs between different criteria [@r37]. Although some criteria can easily be accounted for by setting locked constraints or representation targets [e.g. @r40; @r39], this is not always straightforward [e.g. @r38]. For example, prioritizations often need to balance overall cost with the overall level spatial fragmentation among reserves [@r41; @r42]. Additionally, prioritizations often need to balance overall cost with the overall level of connectivity among reserves [@r43]. Since the best trade-off depends on a range of factors -- such as available budgets, species' connectivity requirements, and management capacity -- finding the best balance can be challenging.

The _prioritizr R_ package provides multi-objective optimization methods to help identify the best trade-offs between different criteria. To achieve this, a conservation planning problem can be formulated with a primary objective (e.g. `add_min_set_objective()`) and penalties (e.g. `add_boundary_penalties()`) that relate to such criteria. When building the problem, the nature of the trade-offs can be specified using certain parameters (e.g. the `penalty` parameter of the `add_boundary_penalties()` function). To identify a prioritization that finds the best balance between different criteria, the trade-off parameters can be calibrated by conducting a multiple-criteria decision analysis [MCDA; reviewed in @r44]. These analyses -- in the context of systematic conservation planning -- typically involve generating a set of candidate prioritizations based on different parameters, measuring their performance according to each of the criteria, and then selecting a prioritization (or set of prioritizations) based on how well they achieve the criteria [@r41; @r42; @r43]. For example, the _Marxan_ decision support tools has a range of parameters (e.g. species penalty factors, boundary length modifier) that are calibrated to produce prioritizations that balance cost, species' representation, and spatial fragmentation [@r45].

The aim of this tutorial is to provide guidance on using multiple-criteria decision analyses to navigate trade-offs among prioritizations generated using the _prioritizr R_ package. Here we will explore a couple of different approaches for generating candidate prioritizations, and methods for finding the best balance between different criteria. Specifically, we will try to generate prioritizations that strike the best balance between total cost and spatial fragmentation (measured as total boundary length). As such, the code used in this vignette will be directly applicable when performing a boundary length calibration analysis.

## Data

Let's load the packages and dataset used in this tutorial. Since this tutorial uses the _prioritizrdata R_ package along with several other _R_ packages (see below), please ensure that they are all installed. This particular dataset comprises two object: `tas_pu` and `tas_features`. Although we will briefly discuss this dataset below, please refer to the _Tasmania Tutorial_ vignette for further details.

```{r, message = FALSE}
# load packages
library(prioritizrdata)
library(prioritizr)
library(dplyr)
library(tibble)
library(scales)
library(ggplot2)
library(topsis)
library(withr)

# load planning unit data
data(tas_pu)

# convert planning units to sf format
tas_pu <- st_as_sf(tas_pu)

# load feature data
data(tas_features)

# print planning unit data
print(tas_pu)

# print feature data
print(tas_features)
```

The `tas_pu` object contains planning units represented as spatial polygons (i.e. a `sf::st_sf()` object). This object has three columns that denote the following information for each planning unit: a unique identifier (`id`), unimproved land value (`cost`), and current conservation status (`locked_in`). Specifically, the conservation status column indicates if at least half the area planning unit is covered by existing protected areas (denoted by a value of 1) or not (denoted by a value of zero).

```{r, fig.width = w, fig.height = h}
# plot map of planning unit costs
plot(tas_pu[, "cost"], main = "Planning unit costs")

# plot map of planning unit statuses
plot(tas_pu[, "locked_in"], main = "Planning unit status")
```

The `tas_features` object describes the spatial distribution of different vegetation communities (using presence/absence data). We will use the vegetation communities as the biodiversity features for the prioritization.

```{r, fig.width = 4.5, fig.height = 4.5}
# plot map of the first four vegetation classes
plot(tas_features[[1:4]], main = paste("Feature", 1:4))
```

We can use this dataset to generate a prioritization. Specifically, we will use the minimum set objective so that the optimization process minimizes total cost. We will add representation targets to ensure that prioritizations cover 17% of each vegetation community. Additionally, we will add constraints to ensure that planning units covered by existing protected areas are selected (i.e. locked in). Finally, we will specify that the conservation planning exercise involves binary decisions (i.e. selecting or not selecting planning units for protected area establishment).

```{r, fig.width = w, fig.height = h, results = "hide"}
# define a problem
p0 <- problem(tas_pu, tas_features, cost_column = "cost") %>%
      add_min_set_objective() %>%
      add_relative_targets(0.17) %>%
      add_locked_in_constraints("locked_in") %>%
      add_binary_decisions()

# print problem
print(p0)

# solve problem
s0 <- solve(p0)

# print result
print(s0)

# create column for making a map of the prioritization
s0$map_1 <- case_when(
  s0$locked_in > 0.5 ~ "locked in",
  s0$solution_1 > 0.5 ~ "priority",
  TRUE ~ "other"
)

# plot map of prioritization
plot(
  s0[, "map_1"], pal = c("purple", "grey90", "darkgreen"),
  main = NULL, key.pos = 1
)
```

We can see that the priority areas identified by the prioritization are scattered across the study area (shown in green). Indeed, none of the priority areas are connect to existing protected areas (shown in purple), and very of them are connect with other priority areas. As such, the prioritization has a high level of spatial fragmentation. If it is important avoid such levels of spatial fragmentation, then we will need to explicitly account spatial fragmentation in the optimization process.

## Preliminary processing

We need to conduct some preliminary processing procedures before we can start the multiple-criteria decision analysis. This is important to help make it easier to find suitable trade-off parameters, and avoid numerical scaling issues that can result in overly long run times (see `presolve_check()` for further information). These processing steps are akin to data scaling (or normalization) procedures that are applied in statistical analysis to improve model convergence.

The first processing procedure involves setting the cost values for all locked in planning units to zero. This is so that the total cost estimates of the prioritization reflects the total cost of establishing new protected areas -- not just total land value. In other words, we want the total cost estimate for a prioritization to reflect the cost of implementing conservation actions. **This procedure is especially important when using the hierarchical approach described below, so that cost thresholds are based on percentage increases in the cost of establishing new protected areas.**

```{r, fig.width = w, fig.height = h}

# set costs for planning units covered by existing protected areas to zero
tas_pu$cost[tas_pu$locked_in > 0.5] <- 0

# plot map of planning unit costs
plot(tas_pu[, "cost"], main = "Planning unit cost")
```

The second procedure involves pre-computing the boundary length data and manually re-scaling the boundary length values. This procedure is important because boundary length values are often very large that, in turn, can cause numerical issues that result in excessive run times (see `presolve_check()` for further details).

```{r}
# generate boundary length data for the planning units
tas_bd <- boundary_matrix(tas_pu)

# manually re-scale the boundary length values
tas_bd@x <- rescale(tas_bd@x, to = c(0.01, 100))
```

After applying these procedures, we can begin conducting multiple-criteria decision analyses.

## Generating candidate prioritizations

Here we will start the multiple-criteria decision analysis by generating a set of candidate prioritizations. Specifically, these prioritizations will be generated using different parameters to specify different trade-offs between the different criteria. Since this tutorial involves navigating trade-offs between the overall cost of a prioritization and the level of spatial fragmentation associated with a prioritization (as measured by total boundary length), we will generate prioritizations using different parameters related to these criteria. We will examine two approaches for generating candidate prioritizations based on multi-objective optimization procedures.
**Although we'll be examining both approaches in this tutorial, you would normally only use one of these approaches when conducting your own analysis**

### Blended approach

The blended approach for multi-objective optimization involves combining separate criteria (e.g. total cost and total boundary length) into a single joint criterion. To achieve this, a trade-off (or scaling) parameter is used to specify the relative importance of each criterion. This approach is the default approach provided by the _prioritizr R_ package. Specifically, each of the functions for adding a penalty to a problem formulation (e.g. `add_boundary_penalties()`) contains a parameter to control the relative importance of the penalties (i.e. the `penalty` parameter). For example, when using the `add_boundary_penalties()` function, setting a high `penalty` value will indicate that it is important to reduce the overall exposed boundary (perimeter) of the prioritization.

The main challenge with the blended approach is identifying a range of suitable `penalty` values to generate candidate prioritizations. If we set a `penalty` value that is too low, then the penalties will have no effect (e.g. boundary length penalties would have no effect on the prioritization). If we set a `penalty` value too high, then the prioritization will effectively ignore the primary objective (e.g. the planning unit cost values would have no effect on the prioritization, so the prioritization will be overly spatially clustered). Thus we need to find a suitable range of `penalty` values before we can generate a set of candidate prioritizations.

We can find a suitable range of `penalty` values by generating a set of preliminary prioritizations. These preliminary prioritizations will be based on different `penalty` values -- similar to the process for generating the candidate prioritizations -- but solved using settings that allow us to generate prioritizations quickly. This is especially important because specifying a `penalty` value that is too high will cause the optimization process to take a very long time to generate a solutions (due to the numerical scaling issues mentioned previously). Here we will avoid run time issues by setting a time limit of 10 minutes per run, and relax the optimality gap to 20%. This is fine because our goal here is to find an estimate for the highest `penalty` value that could result in a suitable prioritization. Indeed, none of these preliminary prioritizations will be considered as candidate prioritizations for the multiple-criteria decision analysis. **Please note that you might need to set a higher time limit, try a smaller `prelim_lower` value, or try a larger `prelim_upper` value when analyzing different datasets.**

```{r, fig.width = 7, fig.height = 5.0, results = "hide"}
# define a problem without boundary penalties
p0 <- problem(tas_pu, tas_features, cost_column = "cost") %>%
      add_min_set_objective() %>%
      add_relative_targets(0.17) %>%
      add_locked_in_constraints("locked_in") %>%
      add_binary_decisions()

# define a range of different penalty values
## note that we use a power scale to avoid focusing on very high penalty values
prelim_lower <- -5   # change this for your own data
prelim_upper <- 2.5  # change this for your own data
prelim_penalty <- round(10^seq(prelim_lower, prelim_upper, length.out = 9), 5)

# generate preliminary prioritizations based on each penalty
## generate preliminary solutions using relaxed gap and time limit
prelim_blended_results <- lapply(prelim_penalty, function(x) {
  s <-
    p0 %>%
    add_boundary_penalties(x, data = tas_bd) %>%
    add_default_solver(gap = 0.2, time_limit = 10 * 60) %>%
    solve()
  s <- data.frame(s = s$solution_1)
  names(s) <- with_options(list(scipen = 30), paste0("penalty_", x))
  s
})

# format results as a single spatial object
prelim_blended_results <- cbind(
  tas_pu, do.call(bind_cols, prelim_blended_results)
)

# create columns for making maps
prelim_blended_results <- bind_cols(
  prelim_blended_results,
  prelim_blended_results %>%
  st_drop_geometry() %>%
  dplyr::select(starts_with("penalty_")) %>%
  mutate_all(function(x) {
    case_when(
      prelim_blended_results$locked_in > 0.5 ~ "locked in",
      x > 0.5 ~ "priority",
      TRUE ~ "other"
    )
  }) %>%
  setNames(gsub("penalty_", "map_", names(.), fixed = TRUE))
)

# plot maps of prioritizations
plot(
  x =
    prelim_blended_results %>%
    dplyr::select(contains("map_")) %>%
    rename_all(function(x) gsub("map_", "Penalty = ", x, fixed = TRUE)),
  pal = c("purple", "grey90", "darkgreen")
)
```

We can see that as the `penalty` value used to generate the prioritizations increases, the spatial fragmentation of the prioritizations decreases. In particular, we can see that a `penalty` value of `r prelim_penalty[8]` result in a single reserve -- meaning this is our best guess of the upper limit. Using this `penalty` value as an upper limit, we will now generate a second series of prioritizations. Critically, this second series of prioritizations will not be generated using with time limit and be generated using a more suitable gap (i.e. 10%).

```{r, fig.width = 7, fig.height = 5.0, results = "hide"}
# define a new set of penalty values
## note that we use a linear scale to explore both low and high penalty values
penalty <- round(seq(1e-5, prelim_penalty[8], length.out = 9), 5)

# generate prioritizations based on each penalty
blended_results <- lapply(penalty, function(x) {
  ## generate solution
  s <-
    p0 %>%
    add_boundary_penalties(x, data = tas_bd) %>%
    solve()
  ## return data frame with solution
  s <- data.frame(s = s$solution_1)
  names(s) <- with_options(list(scipen = 30), paste0("penalty_", x))
  s
})

# format results as a single spatial object
blended_results <- cbind(tas_pu, do.call(bind_cols, blended_results))

# add columns for making maps
blended_results <- cbind(
  blended_results,
  blended_results %>%
  st_drop_geometry() %>%
  dplyr::select(starts_with("penalty_")) %>%
  mutate_all(function(x) {
    case_when(
      blended_results$locked_in > 0.5 ~ "locked in",
      x > 0.5 ~ "priority",
      TRUE ~ "other"
    )
  }) %>%
  setNames(gsub("penalty_", "map_", names(.), fixed = TRUE))
)

# plot maps of prioritizations
plot(
  x =
    blended_results %>%
    dplyr::select(contains("map_")) %>%
    rename_all(function(x) gsub("map_", "Penalty = ", x, fixed = TRUE)),
  pal = c("purple", "grey90", "darkgreen")
)
```

We now have a set of candidate prioritizations generated using the blended approach. The main advantages of this approach is that it is similar calibration analyses used by other decision support tools for conservation (i.e. _Marxan_) and it is relatively straightforward to implement. However, this approach also has a key disadvantage. Because the `penalty` parameter is a unitless trade-off parameter -- meaning that we can't leverage existing knowledge to specify a suitable range of `penalty` values -- we first have to conduct a preliminary analysis to identify an upper limit. Although finding an upper limit was fairly simple for the example datset, it can be difficult to find for more realistic data. In the next section, we will show how to perform generate a set of candidate prioritzations using the hierachical approach -- which does not have this disadvantage.

### Hierarchical approach

The hierarchical approach for multi-objective optimization involves generating a series of incremental prioritizations -- using a different objective at each increment to refine the previous solution -- until the final solution achieves all of the objectives. The advantage with this approach is that we can specify trade-off parameters for each objective based on a percentage from optimality. This means that we can leverage our own knowledge -- or that of decision maker -- when to generate a range of suitable trade-off parameters. As such, this approach does not require us to generate a series of preliminary prioritizations.

This approach is slightly more complicated to implement within the _prioritizr R_ package then blended approach. To start off, we generate an initial prioritization based on a problem formulation that does not consider any penalties. Critically, we will generate this prioritization by solving the problem to optimality (using the `gap` parameter of the `add_default_solver()` function).

```{r, fig.width = w, fig.height = h}
# define a problem without boundary penalties
p1 <- problem(tas_pu, tas_features, cost_column = "cost") %>%
      add_min_set_objective() %>%
      add_relative_targets(0.17) %>%
      add_locked_in_constraints("locked_in") %>%
      add_binary_decisions() %>%
      add_default_solver(gap = 0)

# solve problem
s1 <- solve(p1)

# add column for making a map of the prioritization
s1$map_1 <- case_when(
  s1$locked_in > 0.5 ~ "locked in",
  s1$solution_1 > 0.5 ~ "priority",
  TRUE ~ "other"
)

# plot map of prioritization
plot(
  s0[, "map_1"], pal = c("purple", "grey90", "darkgreen"),
  main = NULL, key.pos = 1
)
```

Next, we will calculate the total cost of the initial prioritization.

```{r, fig.width = w, fig.height = h}
# calculate cost
s1_cost <- eval_cost_summary(p1, s1[, "solution_1"])$cost

# print cost
print(s1_cost)
```

Now we will calculate a series of cost thresholds. These cost thresholds will be calculated by inflating the cost of the initial prioritization by a range of percentage values. Since these values are percentages -- and not unitless values unlike those used in the blended approach -- we can use domain knowledge to specify a suitable range of cost thresholds. For this tutorial, let's assume that it would be impractical -- per our domain knowledge -- to expend more than four times the total cost of the initial prioritization to reduce spatial fragmentation.

```{r}
# calculate cost threshold values
threshold <- s1_cost + (s1_cost * seq(1e-5, 4, length.out = 9))
threshold <- ceiling(threshold)

# print cost thresholds
print(threshold)
```

After generating the cost thresholds, we can use them to generate prioritizations. Specifically, we will generate prioritizations that aim to minimize total boundary length as much as possible -- ignoring the total cost of the prioritizations -- whilst ensuring that the total cost of the prioritization does not exceed a given cost threshold and the other considerations (e.g. locked in constraints). To achieve this, we create a new column in the `tas_pu` object that contains only zero values (called `zeros`) and use this new column to specify the cost data for the prioritizations. Because all the cost values will be zero, it doesn't matter what `penalty` value we use when adding the boundary penalties -- so we will just use a value of 1. Additionally, when it comes to generating the prioritization, we will add linear constraints to ensure that the total cost of the prioritization does not exceed a given cost threshold (using the `add_linear_constraints()` function).

```{r, fig.width = 7, fig.height = 5.0, results = "hide"}
# add a column with zeros
tas_pu$zeros <- 0

# define a problem with zero cost values and boundary penalties
p2 <- problem(tas_pu, tas_features, cost_column = "zeros") %>%
      add_min_set_objective() %>%
      add_boundary_penalties(1, data = tas_bd) %>%
      add_relative_targets(0.17) %>%
      add_locked_in_constraints("locked_in") %>%
      add_binary_decisions()

# generate prioritizations based on each cost threshold
hierarchical_results <- lapply(threshold, function(x) {
  ## generate solution by adding a constraint based on the threshold and
  ## using the "real" cost values (i.e. not zeros)
  s <-
    p2 %>%
    add_linear_constraints(threshold = x, sense = "<=", data = "cost") %>%
    solve()
  ## return data frame with solution
  s <- data.frame(s = s$solution_1)
  names(s) <- paste0("threshold_", x)
  s
})

# format results as a single spatial object
hierarchical_results <- cbind(tas_pu, do.call(bind_cols, hierarchical_results))

# add columns for making maps
hierarchical_results <- cbind(
  hierarchical_results,
  hierarchical_results %>%
  st_drop_geometry() %>%
  dplyr::select(starts_with("threshold_")) %>%
  mutate_all(function(x) {
    case_when(
      hierarchical_results$locked_in > 0.5 ~ "locked in",
      x > 0.5 ~ "priority",
      TRUE ~ "other"
    )
  }) %>%
  setNames(gsub("threshold_", "map_", names(.), fixed = TRUE))
)

# plot maps of prioritizations
plot(
  x =
    hierarchical_results %>%
    dplyr::select(contains("map_")) %>%
    rename_all(function(x) gsub("map_", "Threshold = ", x, fixed = TRUE)),
  pal = c("purple", "grey90", "darkgreen")
)
```

We now have a set of candidate prioritizations generated using the hierarchical approach. This approach can be much faster than the blended approach because it does not require generating a set of prioritizations to identify an upper limit for the `penalty` trade-off parameter. After generating a set of candidate prioritizations, we can then calculate performance metrics to compare the prioritizations.

## Calculating performance metrics

Here we will calculate performance metrics to compare the prioritizations. Since we are conducting a multiple-criteria decision analysis to navigate trade-offs between the total cost of a prioritization and the overall level of spatial fragmentation associated with a prioritization (as measured by total boundary length), we will calculate metrics to assess these criteria. Although we generated two sets of candidate prioritizations in the previous section; for brevity, here we will consider the candidate prioritizations generated using the hierarchical approach. **Please note that you could also apply the following procedures to candidate prioritizations generated using the blended approach.**

```{r}
# calculate metrics for prioritizations
## note that we use p0 and not p1 so that cost calculations are based
## on the cost values and not zeros
hierarchical_metrics <- lapply(
  grep("threshold_", names(hierarchical_results)), function(x) {
    x <- hierarchical_results[, x]
    data.frame(
      total_cost = eval_cost_summary(p0, x)$cost,
      total_boundary_length = eval_boundary_summary(p0, x)$boundary
    )
  }
)
hierarchical_metrics <- do.call(bind_rows, hierarchical_metrics)
hierarchical_metrics$threshold <- threshold
hierarchical_metrics <- as_tibble(hierarchical_metrics)

# preview metrics
print(hierarchical_metrics)
```

After calculating the metrics, let's visualize use them to visualize the relationship between total cost and total boundary length.

```{r, fig.width = 7, fig.height = 5.0}
# create plot to visualize trade-offs
hierarchical_plot <-
  ggplot(
    data = hierarchical_metrics,
    aes(x = total_boundary_length, y = total_cost, label = threshold)
  ) +
  geom_line() +
  geom_point() +
  geom_text(hjust = -0.5) +
  scale_x_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  xlab("Total boundary length of prioritization") +
  ylab("Total cost of prioritization")

# render plot
print(hierarchical_plot)
```

We can see that there is a clear relationship between total cost and total boundary length. It would seem that in order to achieve a lower total boundary length -- and thus lower spatial fragmentation -- the prioritization must have a greater cost. Although we might expect the results to show a smoother curve -- in other words, only Pareto dominant solutions -- this result is expected because we generated candidate prioritizations using the default optimality gap of 10%.

## Selecting a candidate prioritization

Now we need to decide on which candidate prioritization achieves the best trade-off. There are both qualitative and quantitative methods that are available. For example, one qualitative method could involve using the plot to select a candidate prioritization. For instance, a conservation planner might select a prioritization located near the elbow of the relationship (e.g. based on a `threshold` value of `r threshold[3]`). This qualitative method is generally used to help generate prioritizations with the _Marxan_ decision support tool.

```{r, fig.width = 7, fig.height = 5.0}
# create data for plotting
result_data <- hierarchical_metrics

# add column with the column names that contain the candidate prioritizations
result_data$name <- grep(
  "threshold_", names(hierarchical_results), value = TRUE, fixed = TRUE
)

# add column indicating prioritization selected by qualtitative analysis
result_data$result <- "not selected"
result_data$result[3] <- "qualitative"

# create plot to visualize trade-offs and show selected candidate prioritization
result_plot <-
  ggplot(
    data = result_data,
    aes(x = total_boundary_length, y = total_cost, label = threshold)
  ) +
  geom_line() +
  geom_point(aes(color = result), size = 3) +
  geom_text(hjust = -0.4) +
  scale_color_manual(
    values = c("qualitative" = "blue", "not selected" ="black")
  ) +
  # xlab("Total boundary length of prioritization") +
  ylab("Total cost of prioritization") +
  scale_x_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  theme(legend.title = element_blank())

# render plot
print(result_plot)
```

Let's also consider a quantitative approach. Specifically, we will use the Technique for Order of Preference by Similarity to Ideal Solution
(TOPSIS) method [@r46]. This method requires data describing the performance of each prioritization according the different criteria, weights to encode the relative importance of each criteria, and details on whether each criteria should ideally be minimized or maximized. Let's run the analysis, assuming that we equal weighting for total cost and total boundary length.

```{r}
# calculate TOPSIS scores
topsis_results <- topsis(
  decision =
    hierarchical_metrics %>%
    dplyr::select(total_cost, total_boundary_length) %>%
    as.matrix(),
  weights = c(1, 1),
  impacts = c("-", "-")
)

# print results
print(topsis_results)
```

The candidate prioritization with the greatest TOPSIS score is considered to represent the best trade-off between total cost and total boundary length (assuming equal weighting between the two criteria). Let's create a plot to visualize the results from the qualitative and TOPSIS methods.

```{r, fig.width = 7, fig.height = 5.0}
# add column indicating prioritization selected by quantitative analysis
result_data$result[which.max(topsis_results$score)] <- "TOPSIS"

# create plot to visualize trade-offs and show selected candidate prioritization
result_plot <-
  ggplot(
    data = result_data,
    aes(x = total_boundary_length, y = total_cost, label = threshold)
  ) +
  geom_line() +
  geom_point(aes(color = result), size = 3) +
  geom_text(hjust = -0.5) +
  scale_color_manual(
    values = c(
      "qualitative" = "blue",
      "not selected" = "black",
      "TOPSIS" = "red"
    )
  ) +
  xlab("Total boundary length of prioritization") +
  ylab("Total cost of prioritization") +
  scale_x_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  theme(legend.title = element_blank())

# render plot
print(result_plot)
```

We can see that the TOPSIS method selected a different prioritization to the qualitative approach. So, let's compare them.

```{r, fig.width = 7.0, fig.width = 5.0}
# extract column names for creating the prioritizations
qual_name <- result_data$name[[which(result_data$result == "qualitative")]]
quant_name <- result_data$name[[which(result_data$result == "TOPSIS")]]

# create object with results
mcda_results <- cbind(
  tas_pu,
  hierarchical_results %>%
  st_drop_geometry() %>%
  dplyr::select(all_of(
    gsub("threshold_", "map_", c(quant_name, qual_name), fixed = TRUE)
  )) %>%
  setNames(c("Quantitative", "Qualitative"))
)

# plot maps of selected prioritizations
plot(
  x =
    mcda_results %>%
    dplyr::select(Quantitative, Qualitative),
  pal = c("purple", "grey90", "darkgreen")
)
```

How do we which one is best? This is difficult to say. Ideally, additional information could be used to help select a prioritization, such as knowledge on available resources, species' connectivity requirements, and impacts of neighboring land use. However, from a practical perspective, prioritizations generated for an academic context might find the quantitative approach more useful because it is reproducible. Ultimately, both quantitative and qualitative methods are designed to support decision making. This means that they are intended to assist the decision making process, not serve as a replacement.

## Conclusion

Hopefully, this vignette has provided a useful introduction to using multiple-criteria decision analysis to resolve trade-offs in prioritizations. Although we only explored trade-offs between total cost and spatial fragmentation in this tutorial, this analysis could be adapted to explore trade-offs between a wide range of different criteria. For instance, instead of considering total cost as the primary objective, future analyses could explore trade-offs with feature representation (using the `add_min_shortfall_objective()` function). Additionally, instead of spatial fragmentation, future analyses could explore trade-offs that directly relate to connectivity (using the `add_connectivity_penalties()` function) or specific variables of interest -- such as ecosystem intactness or inverse human footprint index [@r47; @r48] -- to inform decision making (using the `add_linear_penalties()` function). Furthermore, after identifying the best `penalty` or `threshold` values to strike a balance between multiple criteria, you could generate a portfolio of prioritizations (e.g. using the `add_gap_portfolio_function()`) to find multiple options for achieving a similar balance. This might be helpful when you need to generate a set of prioritizations that have comparable performance -- in terms of how well they achieve different criteria -- but select different planning units.

## References
